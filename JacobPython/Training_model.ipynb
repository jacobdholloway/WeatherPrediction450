{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import and verify all correct packages are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from DLWP.data import ERA5Reanalysis\n",
    "from DLWP.model import Preprocessor\n",
    "from DLWP.model import DLWPFunctional\n",
    "from DLWP.model.preprocessing import prepare_data_array\n",
    "from DLWP.model import ArrayDataGenerator\n",
    "from DLWP.model import tf_data_generator\n",
    "# from DLWP.custom import CubeSpherePadding2D, CubeSphereConv2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to make sure that the data is present able to be loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_variables(file_path):\n",
    "    try:\n",
    "        with xr.open_dataset(file_path, engine='netcdf4') as ds:  # Specify the engine explicitly\n",
    "            print(\"Variables available in the dataset:\")\n",
    "            print(ds.variables)\n",
    "    except ValueError as e:\n",
    "        print(f\"Failed to open the dataset at {file_path} with error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are finding the data and validating it matches what we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "data_directory = './TrainingData'\n",
    "processed_file_path = os.path.join(data_directory, 'tutorial_z500_t2m.nc')\n",
    "variables = ['z', 't2m']  # Adjusted for actual variable names\n",
    "levels = [500, 0]  # Match variables to levels pair-wise\n",
    "root_directory = './TrainingData'\n",
    "predictor_file = os.path.join(root_directory, 'tutorial_z500_t2m.nc')\n",
    "model_file = os.path.join(root_directory, 'dlwp-cs_tutorial')\n",
    "log_directory = os.path.join(root_directory, 'logs', 'dlwp-cs_tutorial')\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(data_directory, exist_ok=True)\n",
    "    \n",
    "# Initialize ERA5 Reanalysis with custom parameters\n",
    "era = ERA5Reanalysis(root_directory=data_directory, file_id='tutorial')\n",
    "era.set_variables(variables)\n",
    "era.set_levels([l for l in levels if l != 0])  # Exclude single-level from levels for ERA5 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor warning: opening data with default args\n",
      "Generated file path: ./TrainingData/tutorial_t2m.nc\n",
      "Generated file path: ./TrainingData/tutorial_z.nc\n",
      "Preprocessor.data_to_samples: opening and formatting raw data\n",
      "Preprocessor.data_to_samples: creating output file ./TrainingData/tutorial_z500_t2m.nc\n",
      "Preprocessor.data_to_samples: variable/level pair 1 of 2 (z/500)\n",
      "Preprocessor.data_to_samples: calculating mean and std\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacobholloway/Developer/TestData/DLWP/model/preprocessing.py:526: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  lat_dim = 'lat' if 'lat' in ds.dims.keys() else 'latitude'\n",
      "/Users/jacobholloway/Developer/TestData/DLWP/model/preprocessing.py:527: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  lon_dim = 'lon' if 'lon' in ds.dims.keys() else 'longitude'\n",
      "/Users/jacobholloway/Developer/TestData/DLWP/model/preprocessing.py:529: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  ds.dims[lat_dim], ds.dims[lon_dim])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor.data_to_samples: writing batch 1 of 2\n",
      "Preprocessor.data_to_samples: writing batch 2 of 2\n",
      "Preprocessor.data_to_samples: variable/level pair 2 of 2 (t2m/0)\n",
      "Preprocessor.data_to_samples: calculating mean and std\n",
      "Preprocessor.data_to_samples: writing batch 1 of 2\n",
      "Preprocessor.data_to_samples: writing batch 2 of 2\n",
      "Data processed and saved to ./TrainingData/tutorial_z500_t2m.nc_nocoord.nc\n",
      "<xarray.Dataset> Size: 2GB\n",
      "Dimensions:     (lat: 91, lon: 180, sample: 14608, varlev: 2)\n",
      "Coordinates:\n",
      "  * lat         (lat) float32 364B 90.0 88.0 86.0 84.0 ... -86.0 -88.0 -90.0\n",
      "  * lon         (lon) float32 720B 0.0 2.0 4.0 6.0 ... 352.0 354.0 356.0 358.0\n",
      "  * sample      (sample) datetime64[ns] 117kB 2013-01-01 ... 2017-12-31T21:00:00\n",
      "Dimensions without coordinates: varlev\n",
      "Data variables:\n",
      "    predictors  (sample, varlev, lat, lon) float32 2GB dask.array<chunksize=(1, 2, 91, 180), meta=np.ndarray>\n",
      "    mean        (varlev) float32 8B dask.array<chunksize=(2,), meta=np.ndarray>\n",
      "    std         (varlev) float32 8B dask.array<chunksize=(2,), meta=np.ndarray>\n",
      "Attributes:\n",
      "    description:  Training data for DLWP\n",
      "    scaling:      True\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Preprocessor\n",
    "pp = Preprocessor(era, predictor_file=processed_file_path)\n",
    "\n",
    "# Process the data into a series format suitable for DLWP model\n",
    "pp.data_to_series(\n",
    "        batch_samples=10000,\n",
    "        variables=variables,\n",
    "        levels=levels,\n",
    "        pairwise=True,\n",
    "        scale_variables=True,\n",
    "        overwrite=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "# Drop 'varlev' coordinate after processing and save the processed data\n",
    "processed_data = pp.data.drop_vars('varlev')  # Updated method call\n",
    "processed_data.to_netcdf(processed_file_path + '_nocoord.nc')  # Save to new file\n",
    "print(f\"Data processed and saved to {processed_file_path}_nocoord.nc\")\n",
    "\n",
    "    # Optionally, print data for verification\n",
    "print(processed_data)\n",
    "\n",
    "    # Close resources\n",
    "era.close()\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will process the data or something like that I think we are gonna send it into a model or attempt to build a model or some shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data to memory...\n"
     ]
    }
   ],
   "source": [
    "cnn_model_name = 'unet2'\n",
    "base_filter_number = 32\n",
    "min_epochs = 0\n",
    "max_epochs = 10\n",
    "patience = 2\n",
    "batch_size = 64\n",
    "shuffle = True\n",
    "io_selection = {'varlev': ['z/500', 't2m/0']}\n",
    "add_solar = False\n",
    "io_time_steps = 2\n",
    "integration_steps = 2\n",
    "data_interval = 2\n",
    "loss_by_step = None\n",
    "\n",
    "\n",
    "train_set = list(pd.date_range('2013-01-01', '2014-12-31 21:00', freq='3h'))\n",
    "validation_set = list(pd.date_range('2015-01-01', '2016-12-31 21:00', freq='3h'))\n",
    "\n",
    "dlwp = DLWPFunctional(is_convolutional=True, time_dim=io_time_steps)\n",
    "data = xr.open_dataset(predictor_file)\n",
    "train_data = data.sel(sample=train_set)\n",
    "validation_data = data.sel(sample=validation_set)\n",
    "\n",
    "print('Loading data to memory...')\n",
    "train_array, input_ind, output_ind, sol = prepare_data_array(train_data, input_sel=io_selection,\n",
    "                                                            output_sel=io_selection, add_insolation=add_solar)\n",
    "generator = ArrayDataGenerator(\n",
    "dlwp,\n",
    "train_array,\n",
    "rank=3,\n",
    "input_slice=input_ind,\n",
    "output_slice=output_ind,\n",
    "input_time_steps=io_time_steps,\n",
    "output_time_steps=io_time_steps,\n",
    "sequence=integration_steps,\n",
    "interval=data_interval,\n",
    "# insolation_array=None,\n",
    "batch_size=batch_size,\n",
    "shuffle=shuffle,\n",
    "channels_last=True,\n",
    "drop_remainder=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation data to memory...\n",
      "(<tf.Tensor: shape=(10, 2, 2, 2), dtype=float32, numpy=\n",
      "array([[[[0.77889913, 0.48702195],\n",
      "         [0.02883874, 0.8847574 ]],\n",
      "\n",
      "        [[0.03438853, 0.28276706],\n",
      "         [0.55697274, 0.01757421]]],\n",
      "\n",
      "\n",
      "       [[[0.5032731 , 0.8122299 ],\n",
      "         [0.74255896, 0.9718702 ]],\n",
      "\n",
      "        [[0.24370481, 0.7055796 ],\n",
      "         [0.45572832, 0.931594  ]]],\n",
      "\n",
      "\n",
      "       [[[0.34183827, 0.69231576],\n",
      "         [0.20078963, 0.36029312]],\n",
      "\n",
      "        [[0.86367744, 0.96073234],\n",
      "         [0.63765675, 0.97424525]]],\n",
      "\n",
      "\n",
      "       [[[0.37238586, 0.23333843],\n",
      "         [0.80616033, 0.29663953]],\n",
      "\n",
      "        [[0.24276634, 0.3667157 ],\n",
      "         [0.04228544, 0.786869  ]]],\n",
      "\n",
      "\n",
      "       [[[0.2313752 , 0.07551286],\n",
      "         [0.17696486, 0.3855166 ]],\n",
      "\n",
      "        [[0.01171225, 0.7824096 ],\n",
      "         [0.04857456, 0.571879  ]]],\n",
      "\n",
      "\n",
      "       [[[0.05471338, 0.04035477],\n",
      "         [0.700167  , 0.37426528]],\n",
      "\n",
      "        [[0.95976245, 0.28749397],\n",
      "         [0.53857994, 0.70854264]]],\n",
      "\n",
      "\n",
      "       [[[0.48428163, 0.30781627],\n",
      "         [0.2544075 , 0.66548324]],\n",
      "\n",
      "        [[0.7745545 , 0.23591463],\n",
      "         [0.38958535, 0.5129281 ]]],\n",
      "\n",
      "\n",
      "       [[[0.92343545, 0.45116836],\n",
      "         [0.26329768, 0.41228178]],\n",
      "\n",
      "        [[0.87920743, 0.85902405],\n",
      "         [0.34914133, 0.60991   ]]],\n",
      "\n",
      "\n",
      "       [[[0.81090003, 0.44168627],\n",
      "         [0.0852752 , 0.5718163 ]],\n",
      "\n",
      "        [[0.8462067 , 0.907797  ],\n",
      "         [0.4324131 , 0.81395805]]],\n",
      "\n",
      "\n",
      "       [[[0.13530686, 0.07961881],\n",
      "         [0.9765158 , 0.46793103]],\n",
      "\n",
      "        [[0.55138046, 0.24692856],\n",
      "         [0.07460697, 0.9970309 ]]]], dtype=float32)>, <tf.Tensor: shape=(10, 2, 2, 2), dtype=float32, numpy=\n",
      "array([[[[4.7686371e-01, 2.1997663e-01],\n",
      "         [8.4671867e-01, 3.2085752e-01]],\n",
      "\n",
      "        [[9.7882265e-01, 5.7618570e-01],\n",
      "         [5.4754198e-01, 2.2972873e-01]]],\n",
      "\n",
      "\n",
      "       [[[9.2736036e-01, 4.6603149e-01],\n",
      "         [2.2764115e-02, 7.9036027e-01]],\n",
      "\n",
      "        [[5.8933955e-01, 3.8123500e-01],\n",
      "         [3.3531481e-01, 3.7577637e-02]]],\n",
      "\n",
      "\n",
      "       [[[2.6477003e-01, 7.9593700e-01],\n",
      "         [4.7252992e-01, 1.6094470e-01]],\n",
      "\n",
      "        [[3.4214199e-01, 2.0666236e-01],\n",
      "         [8.4610438e-01, 1.0111451e-02]]],\n",
      "\n",
      "\n",
      "       [[[4.5877956e-02, 7.1153307e-01],\n",
      "         [2.2500582e-01, 7.9948664e-01]],\n",
      "\n",
      "        [[7.3629337e-05, 2.1532107e-02],\n",
      "         [7.4651492e-01, 4.8507300e-01]]],\n",
      "\n",
      "\n",
      "       [[[2.9558924e-01, 6.2207389e-01],\n",
      "         [2.6053104e-01, 4.4802409e-01]],\n",
      "\n",
      "        [[8.1684238e-01, 1.6772677e-01],\n",
      "         [7.2018939e-01, 2.0264718e-01]]],\n",
      "\n",
      "\n",
      "       [[[5.3297669e-01, 4.9542278e-01],\n",
      "         [9.6763152e-01, 1.9749057e-01]],\n",
      "\n",
      "        [[8.8922179e-01, 1.9979891e-03],\n",
      "         [6.6262525e-01, 7.2530335e-01]]],\n",
      "\n",
      "\n",
      "       [[[9.2101520e-01, 4.8587066e-01],\n",
      "         [5.0645995e-01, 7.4501354e-01]],\n",
      "\n",
      "        [[3.8228452e-01, 3.3954388e-01],\n",
      "         [5.0384074e-01, 3.2697731e-01]]],\n",
      "\n",
      "\n",
      "       [[[7.7909964e-01, 7.1860170e-01],\n",
      "         [5.5308354e-01, 5.2297735e-01]],\n",
      "\n",
      "        [[9.5998842e-01, 6.4616573e-01],\n",
      "         [5.0075269e-01, 8.8201594e-01]]],\n",
      "\n",
      "\n",
      "       [[[1.2530157e-01, 7.0391124e-01],\n",
      "         [5.6542718e-01, 2.8393951e-01]],\n",
      "\n",
      "        [[6.4223474e-01, 4.7072610e-01],\n",
      "         [8.1159914e-01, 6.5279317e-01]]],\n",
      "\n",
      "\n",
      "       [[[4.9208412e-01, 2.8947592e-01],\n",
      "         [6.1355423e-02, 4.5548135e-01]],\n",
      "\n",
      "        [[4.6252349e-01, 4.7542159e-02],\n",
      "         [8.0017877e-01, 7.2814149e-01]]]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-24 01:47:54.744533: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from DLWP.model.generators import ArrayDataGenerator\n",
    "from DLWP.model.preprocessing import prepare_data_array\n",
    "\n",
    "# Assuming these variables are defined somewhere in your script:\n",
    "# dlwp, validation_data, io_selection, integration_steps, add_solar, \n",
    "# io_time_steps, data_interval, batch_size, channels_last\n",
    "\n",
    "input_solar = (integration_steps > 1 and add_solar)\n",
    "\n",
    "# Prepare the validation data\n",
    "print('Loading validation data to memory...')\n",
    "val_array, input_ind, output_ind, sol = prepare_data_array(\n",
    "    validation_data,\n",
    "    input_sel=io_selection,\n",
    "    output_sel=io_selection,\n",
    "    add_insolation=add_solar\n",
    ")\n",
    "\n",
    "# Instantiate the validation data generator\n",
    "val_generator = ArrayDataGenerator(\n",
    "    dlwp,\n",
    "    val_array,\n",
    "    rank=3,\n",
    "    input_slice=input_ind,\n",
    "    output_slice=output_ind,\n",
    "    input_time_steps=io_time_steps,\n",
    "    output_time_steps=io_time_steps,\n",
    "    sequence=integration_steps,\n",
    "    interval=data_interval,\n",
    "    insolation_array=sol,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    channels_last=True\n",
    ")\n",
    "\n",
    "# Define a simple data generator for demonstration\n",
    "def my_data_generator():\n",
    "    while True:\n",
    "        inputs = np.random.random((10, 2, 2, 2)).astype(np.float32)  # Example input batch\n",
    "        outputs = np.random.random((10, 2, 2, 2)).astype(np.float32)  # Example output batch\n",
    "        yield (inputs, outputs)\n",
    "\n",
    "# TensorFlow dataset from the custom generator\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(None, 2, 2, 2), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None, 2, 2, 2), dtype=tf.float32)\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    generator=my_data_generator,\n",
    "    output_signature=output_signature\n",
    ")\n",
    "\n",
    "# Example usage of the dataset\n",
    "for data in dataset.take(1):\n",
    "    print(data)  # Outputs the generated data for verification\n",
    "\n",
    "# Define a wrapper function for TensorFlow's Dataset API using the generator\n",
    "def tf_data_generator(generator):\n",
    "    def gen():\n",
    "        for inputs, outputs in generator:\n",
    "            yield (inputs, outputs)\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        generator=gen,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 2, 2, 2), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 2, 2, 2), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Instantiate the data generator\n",
    "data_gen_instance = my_data_generator()\n",
    "\n",
    "# Create a TensorFlow dataset for training data\n",
    "tf_train_data = tf_data_generator(data_gen_instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ main_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">402</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ main_input (\u001b[38;5;33mInputLayer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │           \u001b[38;5;34m402\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling2d_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d_1 (\u001b[38;5;33mUpSampling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m)        │             \u001b[38;5;34m6\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">408</span> (1.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m408\u001b[0m (1.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">408</span> (1.59 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m408\u001b[0m (1.59 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, AveragePooling2D, ReLU, ZeroPadding2D ,Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Create TensorFlow datasets for training and validation\n",
    "\n",
    "\n",
    "# Example dimensions for 'cs'\n",
    "cs = (2, 2, 2)  # Example: 128x128 RGB images\n",
    "\n",
    "# Define input\n",
    "main_input = Input(shape=cs, name='main_input')\n",
    "\n",
    "# Define standard convolutional layers\n",
    "conv1 = Conv2D(2, 10, padding='same', activation='relu')(main_input)\n",
    "pool1 = AveragePooling2D((2, 2))(conv1)\n",
    "up1 = UpSampling2D((2, 2))(pool1)\n",
    "relu = ReLU()(up1)\n",
    "\n",
    "# Define output channels for a binary segmentation task\n",
    "output_channels = 2\n",
    "output = Conv2D(output_channels, 1, activation='sigmoid', name='output')(relu)\n",
    "\n",
    "# Create model using the Functional API\n",
    "model = Model(inputs=main_input, outputs=output)\n",
    "\n",
    "model.summary()  # Display the structure of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, History, TensorBoard\n",
    "from tensorflow.keras.layers import ZeroPadding2D\n",
    "from DLWP.custom import EarlyStoppingMin, GeneratorEpochEnd\n",
    "\n",
    "# Directory for saving logs\n",
    "log_directory = './logs'\n",
    "\n",
    "# Callbacks setup\n",
    "checkpoint = ModelCheckpoint('path_to_save_model.keras', save_best_only=True, monitor='val_loss', mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)\n",
    "history = History()\n",
    "early = EarlyStoppingMin(monitor='val_loss' if validation_data is not None else 'loss', min_delta=0., min_epochs=min_epochs, max_epochs=max_epochs, patience=patience, restore_best_weights=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=log_directory, update_freq='epoch')\n",
    "\n",
    "validation_data = tf_val_data  # This should be defined somewhere in your script if used\n",
    "min_epochs = 1\n",
    "max_epochs = 5\n",
    "patience = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "33252456/Unknown \u001b[1m31820s\u001b[0m 957us/step - accuracy: 0.5301 - loss: 0.0849"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected `context` argument in EagerTensor constructor to have a `_handle` attribute but it did not. Was eager Context initialized?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model with the training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_val_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/TestData/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Developer/TestData/.venv/lib/python3.12/site-packages/tensorboard/plugins/scalar/summary_v2.py:89\u001b[0m, in \u001b[0;36mscalar\u001b[0;34m(name, data, step, description)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summary_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscalar_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m[data, step]) \u001b[38;5;28;01mas\u001b[39;00m (tag, _):\n\u001b[1;32m     88\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdebugging\u001b[38;5;241m.\u001b[39massert_scalar(data)\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected `context` argument in EagerTensor constructor to have a `_handle` attribute but it did not. Was eager Context initialized?"
     ]
    }
   ],
   "source": [
    "# Fit the model with the training data\n",
    "model.fit(tf_train_data, validation_data=tf_val_data, epochs=max_epochs, callbacks=[history, early, tensorboard, checkpoint, reduce_lr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
