{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with a DLWP-CS model\n",
    "\n",
    "Finally we will explore using the advanced functionality in DLWP-CS to make a time-series global weather prediction with our trained DLWP-CS model. We will save the prediction to a netCDF file and apply inverse scaling to get physical variables back. Again, I recommend having this model run on a GPU with at least 4 GB of video memory.\n",
    "\n",
    "#### Required packages\n",
    "\n",
    "No new packages are needed here beyond the main DLWP-CS requirements in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "Some user-specified parameters. The `scale_file` contains the mean and standard of the data (which was dropped in the cubed sphere remapping). The `map_files` were produced by the cubed sphere remapping. We can re-use them here so we don't have to generate them again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.pardir)\n",
    "\n",
    "root_directory = '/home/disk/wave2/jweyn/Data'\n",
    "predictor_file = os.path.join(root_directory, 'ERA5', 'tutorial_z500_t2m_CS.nc')\n",
    "scale_file = os.path.join(root_directory, 'ERA5', 'tutorial_z500_t2m.nc')\n",
    "\n",
    "model = os.path.join(root_directory, 'dlwp-cs_tutorial')\n",
    "map_files = ('map_LL91x180_CS48.nc', 'map_CS48_LL91x180.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll resurrect some parameters from the training tutorial. See that notebook for definitions. Note that we omit `data_interval` because we simply select only every 6 hours from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_selection = {'varlev': ['z/500', 't2m/0']}\n",
    "add_solar = True\n",
    "io_time_steps = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the data used for prediction. We'll make weekly forecasts in the test set, initialized at 0 UTC. We need to specify a subset of the data that contains all these initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "validation_set = pd.date_range('2016-12-31', '2018-12-31', freq='6H')\n",
    "validation_set = np.array(validation_set, dtype='datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the initialization dates, the numer of foreward forecast hours, and the time step (could be automated...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.date_range('2017-01-01', '2018-12-31', freq='7D')\n",
    "initialization_dates = xr.DataArray(dates)\n",
    "num_forecast_hours = 5 * 24\n",
    "dt = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the DLWP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.util import load_model, remove_chars, is_channels_last\n",
    "\n",
    "dlwp = load_model(model)\n",
    "\n",
    "# File to save the forecast\n",
    "forecast_file = os.path.join(root_directory, 'forecast_%s.nc' % remove_chars(model.split(os.sep)[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the data and create the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ds = xr.open_dataset(predictor_file)\n",
    "predictor_ds = all_ds.sel(sample=validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model import SeriesDataGenerator\n",
    "\n",
    "sequence = dlwp._n_steps if hasattr(dlwp, '_n_steps') and dlwp._n_steps > 1 else None\n",
    "val_generator = SeriesDataGenerator(dlwp, predictor_ds, rank=3, add_insolation=add_solar,\n",
    "                                    input_sel=io_selection, output_sel=io_selection,\n",
    "                                    input_time_steps=io_time_steps, output_time_steps=io_time_steps,\n",
    "                                    shuffle=False, sequence=sequence, batch_size=32,\n",
    "                                    load=False, channels_last=is_channels_last(dlwp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the estimator and make a prediction\n",
    "\n",
    "We use the handy TimeSeriesEstimator class to intelligently produce a time series forecast. This class depends on the model and the data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.model import TimeSeriesEstimator\n",
    "\n",
    "estimator = TimeSeriesEstimator(dlwp, val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Predicting with model %s...' % model)\n",
    "\n",
    "# Select the samples from the initialization dates. The first \"time\" input to the model is actually one time step earlier\n",
    "samples = np.array([int(np.where(val_generator.ds['sample'] == s)[0]) for s in initialization_dates]) \\\n",
    "    - io_time_steps + 1\n",
    "time_series = estimator.predict(num_forecast_hours // dt, samples=samples, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose if channels_last was used for the model\n",
    "if is_channels_last(dlwp):\n",
    "    time_series = time_series.transpose('f_hour', 'time', 'varlev', 'x0', 'x1', 'x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the variables back to real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scale_file is None:\n",
    "    scale_ds = predictor_ds\n",
    "else:\n",
    "    scale_ds = xr.open_dataset(scale_file)\n",
    "sel_mean = scale_ds['mean'].sel(io_selection)\n",
    "sel_std = scale_ds['std'].sel(io_selection)\n",
    "time_series = time_series * sel_std + sel_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the time series output, when saved to netCDF, is not compatible with TempestRemap. I have yet to figure out why. But there is a function in the DLWP `verify` module that re-formats a time series and produces output that TempestRemap is happy with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.verify import add_metadata_to_forecast_cs\n",
    "\n",
    "fh = np.arange(dt, time_series.shape[0] * dt + 1., dt)\n",
    "time_series = add_metadata_to_forecast_cs(\n",
    "    time_series.values,\n",
    "    fh,\n",
    "    predictor_ds.sel(**io_selection).sel(sample=initialization_dates)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the prediction in cubed sphere format. Drop the string \"varlev\" coordinate for TempestRemap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.drop('varlev').to_netcdf(forecast_file + '.cs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remap the forecast to a latitude-longitude grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DLWP.remap import CubeSphereRemap\n",
    "\n",
    "csr = CubeSphereRemap(to_netcdf4=True)\n",
    "csr.assign_maps(*map_files)\n",
    "csr.convert_from_faces(forecast_file + '.cs', forecast_file + '.tmp')\n",
    "csr.inverse_remap(forecast_file + '.tmp', forecast_file, '--var', 'forecast')\n",
    "os.remove(forecast_file + '.tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was fast, right?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
